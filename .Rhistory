plt_percent_reporting_binned = ggplot(results, aes(percentage, fill = State)) +
geom_histogram(bins=7) +
labs(title = "Only 7 States Are Currently Releasing Any Facility Level Vaccination Data",
subtitle = "Percentage of Facilities Reporting By State",
x = "Percentage",
y = "Count",
caption = "*Data as of March 24, 2021") +
scale_y_continuous(breaks=c(0,1,2)) +
xlim(0,110) +
theme_behindbars() +
theme(text = element_text(size=12)) +
scale_fill_gradientn()
# plot percent reporting with binned percentages into categories
plt_percent_reporting_binned = ggplot(results, aes(percentage, fill = State)) +
geom_histogram(bins=7) +
labs(title = "Only 7 States Are Currently Releasing Any Facility Level Vaccination Data",
subtitle = "Percentage of Facilities Reporting By State",
x = "Percentage",
y = "Count",
caption = "*Data as of March 24, 2021") +
scale_y_continuous(breaks=c(0,1,2)) +
xlim(0,110) +
theme_behindbars() +
theme(text = element_text(size=12)) +
scale_fill_gradient(colours = c("#82CAA4", "#D7790F"))
# plot percent reporting with binned percentages into categories
plt_percent_reporting_binned = ggplot(results, aes(percentage, fill = State)) +
geom_histogram(bins=7) +
labs(title = "Only 7 States Are Currently Releasing Any Facility Level Vaccination Data",
subtitle = "Percentage of Facilities Reporting By State",
x = "Percentage",
y = "Count",
caption = "*Data as of March 24, 2021") +
scale_y_continuous(breaks=c(0,1,2)) +
xlim(0,110) +
theme_behindbars() +
theme(text = element_text(size=12))
plt_percent_reporting_binned
install.packages("sensemakr")
library(sensemakr)
# loads dataset
data("darfur")
# runs regression model
model <- lm(peacefactor ~ directlyharmed + age + farmer_dar + herder_dar +
pastvoted + hhsize_darfur + female + village, data = darfur)
# runs sensemakr for sensitivity analysis
sensitivity <- sensemakr(model = model,
treatment = "directlyharmed",
benchmark_covariates = "female",
kd = 1:3)
View(sensitivity)
# short description of results
sensitivity
# long description of results
summary(sensitivity)
# load tfrrs cleaned and prepped dataset
tffrs = read.csv("C:\Users\Adam\Desktop\code_projects\GitHub\NCAATF\final_data.csv")
# load tfrrs cleaned and prepped dataset
tffrs = read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# load tfrrs cleaned and prepped dataset
tffrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# fit 2x2 DiD model
model <- lm(TIME_SCALED ~ DISTANCE + TREATED + DISTANCE*TREATED)
View(tffrs)
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ DISTANCE + TREATED + DISTANCE*TREATED)
# load tfrrs cleaned and prepped dataset
tfrrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
View(tffrs)
View(tffrs)
View(tffrs)
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ DISTANCE + TREATED + DISTANCE*TREATED, data = tfrrs)
View(tfrrs)
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ TREAT + AFTER + TREAT*AFTER, data = tfrrs)
View(model)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT",
benchmark_covariates = "AFTER",
kd = 1:3)
# implementing sensitivity analysis with sensemakr for the super shoes DiD project
library(sensemakr)
# load tfrrs cleaned and prepped dataset
tfrrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ TREAT + AFTER + TREAT*AFTER, data = tfrrs)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT",
benchmark_covariates = "AFTER",
kd = 1:3)
# implementing sensitivity analysis with sensemakr for the super shoes DiD project
library(sensemakr)
# load tfrrs cleaned and prepped dataset
tfrrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ TREAT + AFTER + TREAT*AFTER, data = tfrrs)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT",
benchmark_covariates = "AFTER",
kd = 1:3)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT",
benchmark_covariates = "AFTER")
model
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT*AFTER",
benchmark_covariates = "AFTER",
kd = 1:3)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT",
benchmark_covariates = "AFTER",
kd = 1:3)
View(model)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "AFTER",
kd = 1:3)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "TREAT",
kd = 1:3)
sensitivity2 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "AFTERTrue",
kd = 1:3)
sensitivity2 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "AFTERTrue")
View(sensitivity1)
View(sensitivity2)
# check results for both specifications
summary(sensitivity1)
summary(sensitivity2)
# check results for both specifications
summary(sensitivity1)
plot(sensitivity1)
# implementing sensitivity analysis with sensemakr for the super shoes DiD project
library(sensemakr)
# load tfrrs cleaned and prepped dataset
tfrrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ TREAT + AFTER + TREAT*AFTER, data = tfrrs)
sensitivity1 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "TREAT",
kd = 1:3)
sensitivity2 <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "AFTERTrue")
# check results for both specifications
summary(sensitivity1)
plot(sensitivity1)
# implementing sensitivity analysis with sensemakr for the super shoes DiD project
library(sensemakr)
# load tfrrs cleaned and prepped dataset
tfrrs <- read.csv("C:\\Users\\Adam\\Desktop\\code_projects\\GitHub\\NCAATF\\final_data.csv")
# fit 2x2 DiD model
model <- lm(TIME_SECS_SCALED ~ TREAT + AFTER + TREAT*AFTER, data = tfrrs)
sensitivity <- sensemakr(model = model,
treatment = "TREAT:AFTERTrue",
benchmark_covariates = "TREAT",
kd = 1:3)
# check results for both specifications
summary(sensitivity)
plot(sensitivity)
ovb_minimal_reporting(sensitivity)
ovb_minimal_reporting(sensitivity, format = latex)
ovb_minimal_reporting(sensitivity, format = "latex")
ovb_minimal_reporting(sensitivity, format = "latex")
rm(list = ls())
library(statnet)
library(tidyverse)
library(magrittr)
library(ggnetwork)
install.packages(c('statnet' ,'ggnetwork'))
install.packages(c("statnet", "ggnetwork"))
install.packages(c("statnet", "ggnetwork"))
install.packages(c("statnet", "ggnetwork"))
install.packages(c("statnet", "ggnetwork"))
rm(list = ls())
library(statnet)
library(tidyverse)
library(magrittr)
library(ggnetwork)
install.packages(c('EpiModel', 'netdiffuseR'))
install.packages(c("EpiModel", "netdiffuseR"))
# Dedicated diffusion packages
library(EpiModel)
library(netdiffuseR)
# First, in our simple example, let's set our parameters ahead of time
n.people <- 100
p.infection <- 0.5
pct.starting.infected <- 0.01
max.time <- 5000
contact.rate <- 0.05 / 2  # prob. of contact between any 2 people in the network
### Step 1: Set up world ###
# Create a random graph, where edges are placed randomly.  This is called a
# Bernoulli or an Erdos-Renyi random graph
set.seed(919)
net <- rgraph(n = n.people, tprob = contact.rate) %>%
symmetrize %>%  # Make symmetric -- doesn't matter who is connected to who
network(matrix.type = "adjacency")  # convert to statnet
rm(list = ls())
library(statnet)
library(tidyverse)
library(magrittr)
library(ggnetwork)
# Dedicated diffusion packages
library(EpiModel)
library(netdiffuseR)
# First, in our simple example, let's set our parameters ahead of time
n.people <- 100
net <- rgraph(n = n.people, tprob = contact.rate)
# Chose a subset of people who are infected initially
infected <- sample(
x = c(T, F),      # people can be infected (T) or susceptible (F)
size = n.people,  # create a vector that is n.people long
replace = T,      # with replacement, so that >1 person can be infected
prob = c(pct.starting.infected, 1 - pct.starting.infected)
)
library("statnet", lib.loc="~/R/win-library/4.0")
detach("package:statnet", unload=TRUE)
library("statnet", lib.loc="~/R/win-library/4.0")
install.packages("statnet")
library("tidyverse", lib.loc="~/R/win-library/4.0")
install.packages("tidyverse")
library("magrittr", lib.loc="~/R/win-library/4.0")
install.packages("magrittr")
install.packages("ggnetwork")
install.packages("EpiModel")
install.packages("netdiffuseR")
rm(list = ls())
library(statnet)
library(tidyverse)
library(magrittr)
library(ggnetwork)
# Dedicated diffusion packages
library(EpiModel)
library(netdiffuseR)
# First, in our simple example, let's set our parameters ahead of time
n.people <- 100
p.infection <- 0.5
max.time <- 5000
pct.starting.infected <- 0.01
contact.rate <- 0.05 / 2  # prob. of contact between any 2 people in the network
contact.rate
### Step 1: Set up world ###
# Create a random graph, where edges are placed randomly.  This is called a
# Bernoulli or an Erdos-Renyi random graph
set.seed(919)
net <- rgraph(n = n.people, tprob = contact.rate) %>%
symmetrize %>%  # Make symmetric -- doesn't matter who is connected to who
network(matrix.type = "adjacency")  # convert to statnet
net
# Chose a subset of people who are infected initially
infected <- sample(
x = c(T, F),      # people can be infected (T) or susceptible (F)
size = n.people,  # create a vector that is n.people long
replace = T,      # with replacement, so that >1 person can be infected
prob = c(pct.starting.infected, 1 - pct.starting.infected)
)
infected
library(igraph)
# Shane's code
# based on https://users.dimi.uniud.it/~massimo.franceschet/R/communities.html
library(igraph)
g = make_graph("Zachary")
# plot the graph
plot(g, vertex.label=NA, vertex.size=10)
# cluster using hierarchical fast greedy method
c1 = cluster_fast_greedy(g)
plot(c1, g)
# membership of each node
membership(c1)
c1$membership # same thing
# number of clusters
k = length(c1)
# size of each cluster
n_members = sizes(c1)
hat_P = matrix(rep(0, 9), ncol = 3)
for(i in 1:k){    #group membership of node i
for(j in 1:k){  #group membership of node j
if (i != j){
hat_P[i,j] = sum(g[(c1$membership == i), (c1$membership == j)])/(n_members[i] * n_members[j])
}
}
}
# Now fill in the diagonal
for(i in 1:k){
hat_P[i,i] = sum(g[(c1$membership == i), (c1$membership == i)])/choose(n_membership[i], 2)
}
print(hat_P)
# http://nickstrayer.me/rstudioconf_sbm/#1
# http://nickstrayer.me/sbmr/
# https://www.rstudio.com/resources/rstudioconf-2020/stochastic-block-models-with-r-statistically-rigerous-clusting-with-rigorous-code/
# fits models using MCMC
library(magrittr)
library(tidyverse)
install.packages("sbmR")
library(sbmR)
install.packages("githubinstall")
githubinstall::gh_install_packages("sbmR")
githubinstall::gh_install_packages("tbilab/sbmr")
githubinstall::gh_install_packages("https://github.com/tbilab/sbmr")
library(devtools)
install_github("tbilab/sbmr")
library(sbmR)
install_github("tbilab/sbmr")
library(sbmR)
library(sbmr)
# simulate a dense network with three blocks and 40 nodes per block
k <- 3      # no. of blocks
n_k <- 40   # nodes/block
network <- sim_basic_block_network(
n_blocks = k,
n_nodes_per_block = n_k
)
visualize_network(network)
# Now find the optimal* starting point for MCMC
#  (*closest to posterior median)
# Set up SBM from data
my_sbm <- create_sbm(network)
# Run agglomerative collapsing
collapse_results <- my_sbm %>%
collapse_blocks(sigma = 1.01)
# Visualize method used to
# choose best state from
# results
collapse_results %>%
visualize_collapse_results(
heuristic = 'delta_ratio'
) +
xlim(0,15) +
geom_vline(xintercept = k)
# Sanity check:
# join model's state after collapse with the
#  true data blocks to see how well the collapsing algorith
#  fit the data
# Model state after merge
merged_state <- my_sbm %>%
get_state() %>%
select(id, parent)
# Join with true block info
left_join(
network$nodes,
merged_state,
by = "id"
) %>%
rename(inferred = parent) %$%
table(inferred, block)
# Sample from posterior:
# Run MCMC sweeps from collapse starting point to
#   investigate the posterior structure.
sweep_results <- my_sbm %>%
mcmc_sweep(num_sweeps = 75,
eps = 0.25,
track_pairs = TRUE)
# Plot Sweep Results
sweep_results$sweep_info %>%
mutate(sweep = 1:n()) %>%
pivot_longer(
c(entropy_delta,
num_nodes_moved)
) %>%
ggplot(aes(x = sweep,
y = value)) +
geom_line() +
facet_grid(name~.,
scales = "free_y")
# Investigate clustering uncertainty:
# Plot the pairwise probability two nodes
#  share block membership
sweep_results$pairing_counts %>%
ggplot(aes(x = node_a,
y = node_b)) +
geom_raster(
aes(fill = proportion_connected)
) +
theme(
axis.text = element_blank(),
axis.ticks = element_blank()
)
# Concensus SNE: visualize the pairwise node-to-node
#  block similarity within a network
sweep_results %>%
visualize_propensity_network()
# ------------------------------ #
# ------------------------------ #
# Continuous latent space models #
# ergmm
# lsm in lvm4net
# lsm in lvm4net
# https://rdrr.io/cran/lvm4net/man/lsm.html
# lsm in lvm4net
# https://rdrr.io/cran/lvm4net/man/lsm.html
# uses variational inferential approach that is less computationally
# lsm in lvm4net
# https://rdrr.io/cran/lvm4net/man/lsm.html
# uses variational inferential approach that is less computationally
#  intensive than the MCMC in the latentnet package
# Shane's code
# based on https://users.dimi.uniud.it/~massimo.franceschet/R/communities.html
library(igraph)
g = make_graph("Zachary")
# plot the graph
plot(g, vertex.label=NA, vertex.size=10)
# cluster using hierarchical fast greedy method
c1 = cluster_fast_greedy(g)
plot(c1, g)
# membership of each node
membership(c1)
c1$membership # same thing
# number of clusters
k = length(c1)
# size of each cluster
n_members = sizes(c1)
hat_P = matrix(rep(0, 9), ncol = 3)
for(i in 1:k){    #group membership of node i
for(j in 1:k){  #group membership of node j
if (i != j){
hat_P[i,j] = sum(g[(c1$membership == i), (c1$membership == j)])/(n_members[i] * n_members[j])
}
}
}
# Now fill in the diagonal
for(i in 1:k){
hat_P[i,i] = sum(g[(c1$membership == i), (c1$membership == i)])/choose(n_membership[i], 2)
}
print(hat_P)
# http://nickstrayer.me/rstudioconf_sbm/#1
# http://nickstrayer.me/sbmr/
# https://www.rstudio.com/resources/rstudioconf-2020/stochastic-block-models-with-r-statistically-rigerous-clusting-with-rigorous-code/
# fits models using MCMC
library(magrittr)
library(tidyverse)
library(sbmr)
# simulate a dense network with three blocks and 40 nodes per block
k <- 3      # no. of blocks
n_k <- 40   # nodes/block
network <- sim_basic_block_network(
n_blocks = k,
n_nodes_per_block = n_k
)
View(network)
visualize_network(network)
# Now find the optimal* starting point for MCMC
#  (*closest to posterior median)
# Set up SBM from data
my_sbm <- create_sbm(network)
install_github("tbilab/sbmR")
install_github("tbilab/sbmR", force = TRUE)
library(sbmR)
library(devtools)
devtools::install("tbilab/sbmR")
install_github("tbilab/sbmR")
remove.packages("sbmr", lib="~/R/win-library/4.0")
install_github("tbilab/sbmR")
library(sbmR)
# http://nickstrayer.me/rstudioconf_sbm/#1
# http://nickstrayer.me/sbmr/
# https://www.rstudio.com/resources/rstudioconf-2020/stochastic-block-models-with-r-statistically-rigerous-clusting-with-rigorous-code/
# fits models using MCMC
library(magrittr)
library(tidyverse)
library(sbmr)
# simulate a dense network with three blocks and 40 nodes per block
k <- 3      # no. of blocks
n_k <- 40   # nodes/block
network <- sim_basic_block_network(
n_blocks = k,
n_nodes_per_block = n_k
)
visualize_network(network)
# Now find the optimal* starting point for MCMC
#  (*closest to posterior median)
# Set up SBM from data
my_sbm <- create_sbm(network)
# Run agglomerative collapsing
collapse_results <- my_sbm %>%
collapse_blocks(sigma = 1.01)
# Simulate a 3 block/cluster network with 15 nodes per block
net <- sim_basic_block_network(n_blocks = 3, n_nodes_per_block = 15)
# Visualize network with color encoding the blocks
visualize_network(net)
# Visualize network with shape encoding the blocks
visualize_network(net, node_shape_col = 'block', node_color_col = 'type')
library(data.table)
# desktop
setwd('C:/Users/Adam/Desktop/code_projects/GitHub/Simulating-Respondent-Driven-Sampling/RRDS_dropbox/RMG_Workers_Study_COVID-19_RRR')
library(data.table)
library(ggplot2)
library(haven)
install.packages("data.table")
version
install.packages("data.table")
library(data.table)
install.packages("data.table")
remotes::install_github('rstudio/blogdown')
package_version("blogdowm")
package_version("blogdown")
packageVersion("blogdown")
length(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5,
6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 10, 10, 10, 10))
length(c(2, 3, 4, 6, 1, 4, 5, 1, 5, 10, 1, 2, 5, 2, 3, 4, 8,
1, 7, 8, 10, 8, 9, 10, 5, 7, 8, 10, 2, 7, 8, 1, 2, 4, 8))
install.packages("ggdag")
install.packages("ggdag")
library(ggdag)
install.packages("dagitty")
setwd('C:\Users\Adam\Desktop\code_projects\GitHub\ard_viz')
setwd('C:/Users/Adam/Desktop/code_projects/GitHub/ard_viz')
