---
title: " "
author: " "
date: ' '
output:
  html_document:
    theme: cosmo
runtime: shiny
---

```{r include=FALSE}
library(visNetwork)
library(igraph)
library(r2d3)
library(kableExtra)
library(dplyr)
library(shiny)
library(tidyverse)
library(shinythemes)
library(DT)
library(png)
library(magick)
```

<br>

# Consistently estimating network statistics using *Aggregated Relational Data* (***ARD***)
#### Breza, Chandrasekhar, Lubold, McCormick, Pan

#### The full paper can be found [here](https://arxiv.org/abs/1908.09881).

<br>

### How does disease spread during a pandemic?

### Or how does a meme go viral on a social media platform?

### Social scientists use ***network data*** to answer these types of questions. 

### However, collecting complete ***network data*** for this type of research is expensive, time-consuming, and often infeasible. 

### ***ARD*** offers a cheaper alternative to researchers who cannot otherwise acquire network data.

### To collect ***ARD***, a researcher asks questions of the form:

<br>

<center> <font size="5">  ***How many people with trait X do you know?*** </font>  </center>

### and uses the responses to estimate unobserved features of the network.

<br>

### Suppose we want to know the ***probability*** that two people are friends given that they are each from distinct, unobserved communities.

### When observable traits are associated with unobserved communities, we can use observations from ARD responses to estimate this ***probability***. 

### The remainder of this primer will illustrate the process with a toy example.

<br>

<center> <font size="5">  ***Washington College Students and Color Preferences: Lavender or Pink?*** </font>  </center>

<br>

### In this simplified example, we have students of three different schools and two different colors. Our goal is to estimate the probability that a given student prefers ***Lavender*** or ***Pink***. The best way to answer this question would be to simply survey each student and ask them their color preference. But in this example, we assume that we cannot ask directly about color preferences. But we can collect data about the network, and combine that with the assumption that color preferences are associated with where students choose to study. 

<br>

### First we begin by generating a network of 10 students (nodes). 

### Each node is randomly assigned to one of two communities: ***Lavender*** or ***Pink***.

<center>
```{r, echo=FALSE}
# images as nodes
# https://datastorm-open.github.io/visNetwork/image_icon.html

set.seed(123)

#dataset
nodes = data.frame(id = 1:10,
                  label = c(rep("Lavender", 5), rep("Pink", 5)),
                  shape = "dot",
                  size = 40, 
                  font.size = 25,
                  align = "center",
                  borderWidth = 2,
                  color.background = c(rep("lavender", 5), 
                                       rep("pink", 5)),
                  color.border = "black",
                  color.highlight.background = "gold",
                  color.highlight.border = "black",
                  shadow = TRUE
                  # title = paste0("</b>Node </p>", 1:10)
)

edges = c()

visNetwork(nodes, edges)
```
</center>

### Next, let's assign a school mascot trait $k$ to each node. ***We assume that node (student) traits (school) are associated with community (color preference), but not perfectly correlated.***  

### For this example, let's assume each node is either a ***UW Husky***, a ***WSU Cougar***, or a ***Gonzaga Bulldog***. In practice, these traits could be other observable characteristics such as age, education, health status, etc.

### A node that prefers ***Lavender*** is more likely to be a ***Husky***, ***Pink*** nodes are more likely to be ***Cougars***, and nodes that are indifferent are most likely to be ***Bulldogs***.

<center>
```{r, echo=FALSE}
# images as nodes
# https://datastorm-open.github.io/visNetwork/image_icon.html

uw = "https://raw.githubusercontent.com/avisokay/ard_viz/master/uw.png"
wsu = "https://raw.githubusercontent.com/avisokay/ard_viz/master/wsu.png"
gu = "https://raw.githubusercontent.com/avisokay/ard_viz/master/gu.png"

#dataset
nodes = data.frame(id = 1:10,
                  # label = as.character(1:10),
                  shape = "circularImage",
                  image = c(gu, uw, uw, uw, wsu, uw, wsu, gu, wsu, wsu),
                  size = 40,
                  font.size = 25,
                  align = "center",
                  borderWidth = 2,
                  color.background = c(rep("lavender", 5), 
                                       rep("pink", 5)),
                  color.border = "black",
                  color.highlight.background = "gold",
                  color.highlight.border = "black",
                  shadow = FALSE
                  # title = paste0("</b>Node </p>", 1:10)
)

edges = c()

visNetwork(nodes, edges)
```
</center>

### Finally, we assume there is ***community homophily*** in this network, such that the ***probability*** of node $i$ knowing node $j$ (denoted as $p_{ij}$), is ***greater*** if $i$ and $j$ are in the ***same community***. 

### In other words, nodes who prefer ***Lavender*** are more likely to connect with nodes who also prefer ***Lavender***, and same for ***Pink*** nodes. 

### Now that we have generated nodes with color preferences, assigned school mascot traits $k$, and formed edges, we will call this ***complete network $G$***. 

<center>
```{r, echo=FALSE}

edges = data.frame(from = c(1, 2, 3, 3, 4, 4, 5, 6, 6, 7, 8, 9, 8),
                  to   = c(3, 3, 4, 5, 5, 6, 7, 2, 8, 8, 9, 10, 10))

visNetwork(nodes, edges)
```
</center>

### $G$ is what a researcher would oberve ***if*** they could collect ***complete*** network data.

### In this example however, the researcher can only directly observe trait ***k*** (school), not color preferences or edges between individual nodes. We call this ***observed network $G^{*}$***.

<center>
```{r, echo=FALSE}

#dataset
nodes$color.background = "white"

edges = c()

visNetwork(nodes, edges)
```
</center>

# 3. Gathering *Aggregated Relational Data* (***ARD***)

### Recall that ***ARD*** is collected by asking nodes in a network questions of the form *How many people with trait X do you know?* 

### In this example, the researcher would ask each node: How many ***UW Huskies*** do you know? How many ***WSU Cougars***? How many ***Gonzaga Bulldogs***?

<br> 
<center> <font size="5">  ***Click on a node to see each who it knows!*** </font>  </center>

<center>
```{r, echo=FALSE}
#dataset
nodes = data.frame(id = 1:10,
                  label = as.character(1:10),
                  shape = "circularImage",
                  image = c(gu, uw, uw, uw, wsu, uw, wsu, gu, wsu, wsu),
                  size = 40,
                  font.size = 25,
                  align = "center",
                  borderWidth = 2,
                  color.background = "white",
                  color.border = "black",
                  color.highlight.background = "gold",
                  color.highlight.border = "black",
                  shadow = FALSE
                  # title = paste0("</b>Node </p>", 1:10)
)

edges = data.frame(from = c(1, 2, 3, 3, 4, 4, 5, 6, 6, 7, 8, 9, 10),
                  to   = c(3, 3, 4, 5, 5, 6, 7, 2, 8, 8, 9, 10, 8))

# how many each node knows data
ARD = data.frame(rbind(c(1, 0, 1, 0),
                       c(2, 0, 2, 0),
                       c(3, 1, 2, 1),
                       c(4, 0, 2, 1),
                       c(5, 0, 2, 1),
                       c(6, 1, 2, 0),
                       c(7, 1, 0, 1),
                       c(8, 0, 1, 3),
                       c(9, 1, 0, 1),
                       c(10, 1, 0, 1)))
colnames(ARD) = c("Node", "Bulldog", "Husky", "Cougar")
ui = fluidPage(
    visNetworkOutput("network_proxy", height = "600px"),
    dataTableOutput("nodes_data_from_shiny", width="100%")
)
server = function(input, output, session) {
  # highlight selected node and contacts
  output$network_proxy = renderVisNetwork({
    visNetwork(nodes, edges) %>%
      visOptions(highlightNearest=TRUE) %>%
      
      visEvents(select = "function(nodes) {
                Shiny.onInputChange('current_node_id', nodes.nodes);
                ;}")
  })
  
  # display output from ARD table based on node selection
  output$nodes_data_from_shiny = renderDT({
    info = data.frame(ARD)
    
    info[ARD$Node == input$current_node_id, ]
  }, 
  
  # only display table, not search or page toggle
  options = list(dom = 't'),
  rownames = FALSE)
}
shinyApp(ui, server, options = list(height = 725))
```
</center>

### Asking each node $i$ in $G^{*}$ how many of their neighbors have a given trait $k$  produces a single ***ARD*** response $y_{ik}$.

<br>

### For example, node 4 knows no ***Bulldogs***, two ***Huskies***, and one ***Cougar***, so

<br>

<center> <font size="5">  $y_{4,Bulldog} = 0, \;\;\;\;\;\;\;\;\;\;\; y_{4,Husky} = 2, \;\;\;\;\;\;\;\;\;\;\; y_{4,Cougar} = 1$ <font> <center/>

<br>

### Node 8 also knows no ***Bulldogs***, but knows one ***Husky*** and three ***Cougars***, so

<br>

<center> <font size="5">  $y_{8,Bulldog} = 0, \;\;\;\;\;\;\;\;\;\;\; y_{8,Husky} = 1, \;\;\;\;\;\;\;\;\;\;\; y_{8,Cougar} = 3$ <font> <center/>

<br>

### and so on...

### Repeating this process and aggregating for all $y_{ik}$ yields the following ***ARD*** table which we call $Y$ below.

<br>

<center>
```{r, echo = FALSE}
ARD %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = T)
```
</center>

# 4. Estimating Community Structure using ***ARD***

### Now we can use ***ARD*** responses $Y$ we gathered above to estimate the probability that a given node is a member of unobserved latent communities <b><i>1</i></b> or <b><i>2</i></b>.

### Please refer to the [full paper](https://arxiv.org/abs/1908.09881) for further details on how we compute estimates using the ARD data.

### First, we use k-means clustering to classify nodes into unobserved latent communities <b><i>1</i></b> and <b><i>2</i></b> based on their ***ARD*** responses $y_{ik}$. 

### In our example, since ***Huskies*** know mostly other ***Huskies***, they get clustered together while ***Bulldogs*** don't know other ***Bulldogs***, so they are not clustered together. 

<center>
```{r, echo = FALSE,out.width="850.px"}

knitr::include_graphics("communities_uw.PNG")

```
</center>

### Refer to page seven of our [full paper](https://arxiv.org/abs/1908.09881) for details about our proposed clustering algorithm.

### Once we have assigned nodes to communities <b><i>1</i></b> and <b><i>2</i></b>, we calculate the fraction of each trait in each community. 

### So in our example we have community <b><i>1</i></b> as

<br>

<center>
```{r echo = FALSE,out.width="850.px"}

knitr::include_graphics("probability_2_uw.png")

```
</center>

<br>

### and community <b><i>2</i></b> as

<br>

<center>
```{r echo = FALSE,out.width="850.px"}

knitr::include_graphics("probability_1_uw.png")

```
</center>

<br>

### Next we take ARD responses $y_i$ and disaggregate by each trait $k$ into $y_{ik}$. 

<br>

<center>
```{r, echo = FALSE,out.width = "600px"}

knitr::include_graphics("disaggregate_uw.PNG")

```
</center>

<br>

### We now have all the pieces needed to compute our estimate.

<ol>
  <li>ARD responses (node 4 knows zero Bulldogs, node 8 knows three Cougars, etc.)</li>
  <li>The conditional Probability of having trait k, given K-means community assignment (80% Huskies in community <b><i>1</i></b>, for example.) </li>
  <li>The total number of nodes in each K-means community (5 for each)</li>
</ol>

### For each node (student) $y_i$, and each trait (school mascot) $k$, we multiply their associated ARD response by the conditional probability of having trait $k$ and then dividing that product by the number of nodes in the associated community. 

### Then we take the sum over the traits for each student. 

### Let's walk through the calculation for $y_8$ (node 8). For ARD responses, $y_{8,Bulldog}$ = 0, $y_{8,Husky}$ = 1, and $y_{8,Cougar }$ = 3. 

### The conditional probability of being a ***Bulldog*** given assignment to Community <b><i>2</i></b> is 0.2. Likewise, for ***Huskies*** it is 0, and for ***Cougars*** it is 0.8. 

### And the number of nodes in <b><i>2</i></b> is 5. 

### So for $y_{8,Bulldog}$, we have $\frac{0 \; * \; 0.2}{5} = 0$,
### plus $y_{8,Husky} = \frac{1 \; * \; 0}{5} = 0$, 
### plus $y_{8,Cougar} = \frac{3 \; * \; 0.8}{5} = 0.48$
### which yields a total of 0.48 for $y_8$. 

### We do this for each node. 

$y_{i,Husky} = \frac{1 \; * \; 0}{5} = 0$

### Let's consider how we compute $\sum_{k} \frac{y_{ik} \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; k)}{n\hat{c}^{\prime}}$ for node $y_8$. 

<br>

### Recall that from $Y$, we have the following $y_{ik}'s$

<br>

<center> <font size = 5> $y_{8,Bulldog} = 0, \;\;\;\;\;\;\;\;\;\;\; y_{8,Husky} = 1, \;\;\;\;\;\;\;\;\;\;\; y_{8,Cougar} = 3$ <font> </center>

### Because there are 5 nodes each in $n\hat{c}$ and $n\hat{c}^{\prime}$ and we surveyed all nodes, 

<br>

<center> <font size = 5> $n\hat{c} = 5, \;\;\;\;\;\;\;\;\;\;\; n\hat{c}^{\prime} = 5$ <font> </center>

### From the clustering we have 

<br>

<center> <font size = 5> $\mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Bulldog) = 0.2$ <font> </center>

<br>

<center> <font size = 5> $\mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Husky) = 0$ <font> </center>

<br>

<center> <font size = 5> $\mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Cougar)  \;\;= 0.8$<font> </center>

<br>

### For $i$ = 8 and $k$ = ***Bulldog***,

<br>

<center> <font size = 5> $\frac{y_{8, Bulldog} \; * \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Bulldog)}{n\hat{c}^{\prime}} = \frac{0 \; * \; 0.2}{5} = 0$ <font> </center>

### For $i$ = 8 and $k$ = ***Husky***,

<br>

<center> <font size = 5> $\frac{y_{8, Husky} \; * \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Husky)}{n\hat{c}^{\prime}} = \frac{1 \; * \; 0}{5} = 0$ <font> </center>

<br>

### And for $i$ = 8 and $k$ = ***Cougar***, 

<br>

<center>
```{r, echo = FALSE,out.width = "800px"}

knitr::include_graphics("scaling2.png")

```
</center>

<br>

### dividing by $n\hat{c}^{\prime} = 5$ gives

<br>

<center> <font size = 5> $\frac{y_{8, Cougar} \; * \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; Cougar)}{n\hat{c}^{\prime}} = \frac{3 \; * \; 0.8}{5} = 0.48$ <center/>

<br>

### Then we take the sum of each computed $\frac{y_{ik} \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; k)}{n\hat{c}^{\prime}}$ for each trait $k$. 

<br>

### So for $y_8$, $\sum_{k} \frac{y_{8k} \; \mathbb{P}(community \; = \; \hat{c}^{\prime} \; | \; trait \; = \; k)}{n\hat{c}^{\prime}} = 0 + 0 + 0.48 = 0.48$

<br>

### Finally, to calculate $\hat{P}cc^{\prime}$, we do the above calculation for each node $i \in \hat{c}$ and sum over the sample size of <b><i>1</i></b>, n<b><i>1</i></b>. 

<br>

<center>
```{r echo=FALSE}
Y = data.frame(rbind(c(0, 1, 0),
                       c(0, 2, 0),
                       c(1, 2, 1),
                       c(0, 2, 1),
                       c(0, 2, 1),
                       c(1, 2, 0),
                       c(1, 0, 1),
                       c(0, 1, 3),
                       c(1, 0, 1),
                       c(1, 0, 1)))
colnames(Y) = c("Bulldog", "Triangle", "Cougar")
Y = as.matrix(Y)

hat_c_prime = c(0.2, 0, 0.8)

result = Y%*%hat_c_prime
result_scaled = result/5

results = data.frame(Node = c(2,3,4,6,8),
                     Probability = c(result_scaled[2],
                                     result_scaled[3],
                                     result_scaled[4],
                                     result_scaled[6],
                                     result_scaled[8])
                     )

names(results) = c("$i \\in \\hat{c}$", "$Probability$")

results %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE,
                position = "center")
```
</center>

<br>

### In our toy example, we estimate $\hat{P}cc^{\prime} = \frac{1}{5} \; (0 + 0.20 + 0.16 + 0.04 + 0.48) = 0.176$

<br> 

<br>

<br>

<br>

<br>

<br>

<br>




